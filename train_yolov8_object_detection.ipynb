{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdRhABQLRlpz"
   },
   "source": [
    "## Before you start\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gl_XK04IRhU5",
    "outputId": "0b08de66-d96f-4ebc-a859-b15bc441188b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 24 17:29:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Install YOLOv8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ie5uLDH4uzAp",
    "outputId": "961c9cef-7d11-4edf-837d-abaaf9514b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 17022, done.\u001b[K\n",
      "remote: Total 17022 (delta 0), reused 0 (delta 0), pack-reused 17022 (from 1)\u001b[K\n",
      "Receiving objects: 100% (17022/17022), 15.62 MiB | 14.23 MiB/s, done.\n",
      "Resolving deltas: 100% (11694/11694), done.\n"
     ]
    }
   ],
   "source": [
    "# clone YOLOv8 repository\n",
    "%cd /content\n",
    "!git clone https://github.com/ultralytics/yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "5731d315-267d-4419-8828-9611fa00d6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mSetup complete. Using torch 2.5.0+cu121 _CudaDeviceProperties(name='NVIDIA A100-SXM4-40GB', major=8, minor=0, total_memory=40513MB, multi_processor_count=108, uuid=08c7fc9b-20c1-7d2b-9463-3e1f225a3c05, L2_cache_size=40MB)\n"
     ]
    }
   ],
   "source": [
    "# install dependencies as necessary\n",
    "!pip install -r requirements.txt\n",
    "!pip uninstall wandb -qy  # deprecated dependency\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "\n",
    "# clear_output()\n",
    "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxpTCstRTcQZ"
   },
   "source": [
    ">  **Tip:** The examples below work even if you use our non-custom dataset. However, you won't be able to deploy the model to Roboflow. To do that, create a custom dataset as described above or fork (copy) one into your [workspace](https://app.roboflow.com/) from [Universe](https://universe.roboflow.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Knxi2ncxWffW",
    "outputId": "3591420b-cae2-48dd-a462-47ceb224932e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/yolov5\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "visit https://app.roboflow.com/auth-cli to get your authentication token.\n",
      "Paste the authentication token here: 路路路路路路路路路路\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Generating version still in progress. Progress: 96.89%\n",
      "Exporting format yolov5pytorch in progress : 85.0%\n",
      "Version export complete for yolov5pytorch format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in cash-counter-3 to yolov5pytorch:: 100%|| 124664/124664 [00:08<00:00, 14753.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to cash-counter-3 in yolov5pytorch:: 100%|| 6126/6126 [00:00<00:00, 7592.97it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd /content/yolov8\n",
    "!pip install -q roboflow==1.1.48\n",
    "\n",
    "import roboflow\n",
    "roboflow.login()\n",
    "\n",
    "rf = roboflow.Roboflow()\n",
    "project = rf.workspace(\"model-examples\").project(\"cash-counter-p08xm\")\n",
    "dataset = project.version(3).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUOiNLtMP5aG"
   },
   "source": [
    "# Train Custom YOLOv8 Detector\n",
    "\n",
    "### Next, we'll fire off training!\n",
    "\n",
    "\n",
    "Here, we are able to pass a number of arguments:\n",
    "- **img:** define input image size\n",
    "- **batch:** determine batch size\n",
    "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
    "- **data:** set the path to our yaml file\n",
    "- **cfg:** specify our model configuration\n",
    "- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n",
    "- **name:** result names\n",
    "- **cache:** cache images for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NcFxRcFdJ_O"
   },
   "outputs": [],
   "source": [
    "# train yolov8s on custom data for 25 epochs\n",
    "# time its performance\n",
    "%%time\n",
    "%cd /content/yolov8/\n",
    "!python train.py \\\n",
    "  --img 416 \\\n",
    "  --batch 16 \\\n",
    "  --epochs 25 \\\n",
    "  --data {dataset.location}/data.yaml \\\n",
    "  --weights yolov8s.pt \\\n",
    "  --name yolov8s_results  \\\n",
    "  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJVs_4zEeVbF"
   },
   "source": [
    "# Evaluate Custom YOLOv8 Detector Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KN5ghjE6ZWh"
   },
   "source": [
    "You can view the training graphs associated with a training job in the `/content/yolov8/runs/train/yolov8s_results/results.png` folder.\n",
    "\n",
    "Training losses and performance metrics are also saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov8s_results`.\n",
    "\n",
    "Note from Glenn: Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiW1rFNV79v4"
   },
   "outputs": [],
   "source": [
    "from utils.plots import plot_results  # plot results.txt as results.png\n",
    "Image(filename='/content/yolov8/runs/train/yolov8s_results/results.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLI1JmHU7B0l"
   },
   "source": [
    "### Curious? Visualize Our Training Data with Labels\n",
    "\n",
    "After training starts, view `train*.jpg` images to see training images, labels and augmentation effects.\n",
    "\n",
    "Note a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Glenn Jocher and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "PF9MLHDb7tB6",
    "outputId": "248b60ee-b878-473e-de8d-1f614256a463"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72de90a1763f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/yolov5/runs/train/yolov5s_results/val_batch0_labels.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "Image(filename='/content/yolov8/runs/train/yolov8s_results/val_batch0_labels.jpg', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3qM6T0W53gh"
   },
   "source": [
    "# Run Inference With Trained Weights\n",
    "\n",
    "Next, we can run inference with a pretrained checkpoint on all images in the `test/images` folder to understand how our model performs on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIEwt5YLeQ7P",
    "outputId": "d0537f18-18b1-4aae-b8e3-4e6143f22961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mtrain\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "# trained weights are saved by default in our weights folder\n",
    "%ls runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4SyOWS80qR32",
    "outputId": "627e7c4f-b765-44da-8a55-50986bc95f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best.pt  last.pt\n"
     ]
    }
   ],
   "source": [
    "%ls runs/train/yolov8s_results/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UVlvGf96PIm"
   },
   "outputs": [],
   "source": [
    "%cd /content/yolov8/\n",
    "!python detect.py --weights runs/train/yolov8s_results/weights/best.pt --img 416 --conf 0.35 --source {dataset.location}/test/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNmIV9mX6R2t"
   },
   "outputs": [],
   "source": [
    "project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"/content/yolov8/runs/train/yolov8s_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZissIB9g6VJr"
   },
   "outputs": [],
   "source": [
    "#Run inference on your model on a persistant, auto-scaling, cloud API\n",
    "\n",
    "#load model\n",
    "model = project.version(dataset.version).model\n",
    "\n",
    "#choose random test set image\n",
    "import os, random\n",
    "test_set_loc = dataset.location + \"/test/images/\"\n",
    "random_test_image = random.choice(os.listdir(test_set_loc))\n",
    "print(\"running inference on \" + random_test_image)\n",
    "\n",
    "pred = model.predict(test_set_loc + random_test_image, confidence=40, overlap=30).json()\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
